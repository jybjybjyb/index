---
layout: post
title: Net design
date: 2021-5-11
author: 
tags: [cv, note]
comments: true
toc: true
pinned: False
---

<!-- more -->

# 结构优化

## normalization


![](https://pic1.zhimg.com/80/v2-9aa072f297f0b0ff612c9932e9b0ce08_1440w.jpg)

1. BN
1. IN
1. LN



### Batch-normalization BN

- Batch Normalization是2015年一篇论文中提出的数据归一化方法，往往用在深度神经网络中激活层之前。
- 其作用可以加快模型训练时的收敛速度，使得模型训练过程更加稳定，避免梯度爆炸或者梯度消失。
- 并且起到一定的正则化作用，几乎代替了Dropout。
- 归一化的目的：将数据规整到统一区间，减少数据的发散程度，降低网络的学习难度。
- BN的精髓在于归一之后，使用 [公式] 作为还原参数，在一定程度上保留原数据的分布。
- 
- ![](https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%7D%26Input%3AB%3D%5C%7Bx_%7B1...m%7D%5C%7D%3B%5Cgamma+%2C%5Cbeta+%28parameters%5C+to%5C+be%5C+learned%29%5C%5C+%26Output%3A%5C%7By_i%3DBN_%7B%5Cgamma+%2C%5Cbeta%7D%28x_i%29%5C%7D+%5Cend%7Balign%7D%5C%5C+%5Cmu_%7BB%7D+%5Cleftarrow%5Cfrac%7B1%7D%7Bm%7D%5Csum_%7Bi%3D1%7D%5E%7Bm%7D%7Bx_i%7D%5C%5C+%5Csigma_%7BB%7D%5E2%5Cleftarrow%5Cfrac%7B1%7D%7Bm%7D%5Csum_%7Bi%3D1%7D%5Em%28x_i-%5Cmu_B%29%5E2%5C%5C+%5Ctilde%7Bx%7D_i%5Cleftarrow%5Cfrac%7Bx_i-%5Cmu_B%7D%7B%5Csqrt%7B%5Csigma_B%5E2%2B%5Cepsilon%7D%7D%5C%5C+y_i%5Cleftarrow%5Cgamma%5Ctilde%7Bx%7D_i%2B%5Cbeta%5C%5C+)

- 训练时，均值、方差分别是该批次内数据相应维度的均值与方差
- 推理时，均值、方差是基于所有批次的期望计算所得


![](https://pic3.zhimg.com/v2-0a9861c516dbb2fe19d48ca8b2bb87ee_b.jpg)

- 收敛速率增加
- 可以达到更好的精度
- 在目标检测算法中，BN已经成为了标配。比如Yolov3引入了BN后，mAP提升了两个百分点。
- BN同时起到了正则化作用，防止模型在训练集上过拟合，通常有BN的网络不再需要Dropout层。


### Instance-normalization IN

- IN本质上是一种Style Normalization，它的作用相当于把不同的图片统一成一种风格。
- 图像的风格（artistic style）就是特征图各个feature channel跨空间的统计信息，比如mean和variance。迁移各个channel的mean和variance就可以实现风格迁移，为此他们提出了Adaptive Instance Normalization。

**风格迁移**
1. Gatys等人[2]的首先提出了一种代表性的图像风格迁移方法，使用Gram矩阵作为风格的数学表示：假设一个CNN（如VGG19）的某一层 [公式] 在一张图片上提取出特征图 [公式] ，那么这张图片的artistic style可以表示为特征图上的Gram矩阵
1. Gram矩阵本质上算的是特征图各个channel的相关性，即哪些channel倾向于同时激活，哪些channel倾向于此消彼长，是一种特征channel的统计信息
1. conditional instance normalization(CIN)，Dumoulin等人的方法迁移有限种的风格，想迁移新的的风格则需要训练新的模型。
1. 黄勋等人[1]提出adaptive instance normalization(AdaIN)，即直接用一张图像算出[公式] 和 [公式]，作用到另一张图像上，实现风格迁移，这样只要给两张图片就可以迁移，不需要训练新的网络了。


![](https://pic2.zhimg.com/v2-ab2168cc21075a780416abd508e8f84d_b.jpg)
![](https://images.weserv.nl/?url=https://i0.hdslb.com/bfs/article/db62959941c648c94d7688d699a09e719327e680.png)

- IN相较于BN能够加速收敛，而BN+其他style normalization[6]方法后也能加速收敛，所以IN does perform a kind of style normalization。
- 另外，既然IN和BN都会统一图片的风格，那么在Generator里加IN或BN应该是不利于生成风格多样的图片的，论文中也进行了展示：
- 

## 残差连接

# Opt优化



# Loss函数

## 设计原则
1. 能够表示网络输出和待分割目标的相似程度
1. Loss的计算过程是可导的，可以进行误差反传

## Cross-Entropy (CE)
Cross-Entropy和它的一些改进说起，这一类Loss函数可以叫做Pixel-Level的Loss，因为他们都是把分割问题看做对每个点的分类

### wCE (weighted cross-entropy）
1. 对边界像素这些难学习的像素加大权重（pixel-weight）
1. 做类层面的加权（class-weight），比如对前景像素加大权重，背景像素减小权重，或者不同的类别按照其所占像素的比例，分配权重，占比小的权重大一些，占比大的权重小一些，解决样本不平衡的问题。

### Focal Loss
1. 使难分类样本权重大，易分类样本权重小。至于哪些是难分类样本哪些是易分类样本，都由网络的输出和真实的偏差决定。这就实现了**网络自适应调整**。类比我们学知识，难学习的内容，我们同样时间学的不好自己知道，我就会自觉的花更多精力去学习。以前的神经网络没有识别难易任务自动分配精力的方式，focal loss带来了这种自适应反馈。

### Dice Loss
1. Dice系数是分割效果的一个评判指标，其公式相当于预测结果区域和ground truth区域的交并比，所以它是把一个类别的所有像素作为一个整体去计算Loss的。因为Dice Loss直接把分割效果评估指标作为Loss去监督网络，不绕弯子，而且计算交并比时还忽略了大量背景像素，解决了正负样本不均衡的问题，所以收敛速度很快。


## 分割任务

**语义分割中一般用交叉熵来做损失函数，而评价的时候却使用IOU来作为评价指标**
1. 首先采用交叉熵损失函数，而非 dice-coefficient 和类似 IoU 度量的损失函数，一个令人信服的愿意是其梯度形式更优：
1. 直接采用 dice-coefficient 或者 IoU 作为损失函数的原因，是因为分割的真实目标就是最大化 dice-coefficient 和 IoU 度量. 而交叉熵仅是一种代理形式，利用其在 BP 中易于最大化优化的特点.
1. **Dice Loss** 训练误差曲线非常混乱，很难看出关于收敛的信息。尽管可以检查在验证集上的误差来避开此问题。

### Dics Loss

- 医学图像分割任务中常用的 Dice 损失函数
- Dice 相似系数（DSC，Dice Similarity Coefficient）
- 

### T-test

# 预处理


## Patch Size and Batch Size

在三维医学图像的处理中，显存不足是经常容易遇到的问题。而训练过程中，显存占用与模型的输入patch size，batch size以及网络的结构相关。然而，patch size越大，所需要的模型越大。因此，在抉择时，主要在patch size与batch size之间进行权衡。nnUNet中优先考虑patch size，保证模型能基于更多的信息来进行推理。但batch size最小值应该为2，需要保证训练过程中优化过程的鲁棒性。最后，在保证patch size的前提下，如果显存有多余的，再对应增加batch size。因为batch size大部分情况都比较小，所以之前固定设计中没有用BN而使用Instance Norm。

而最优patch size的决定，采用迭代的方式进行。将batch size先固定为2，然后设定一个较大的patch size初始值，再根据当前patch size来设计网络结构。之后根据显存占用情况，不断的迭代调整patch size。

## target spacing

target spacing指的是数据集在经过重采样之后，每张图像所具有的相同的spacing值，即每张图像的每个体素所代表的实际物理空间大小。
## 体积阈值化
为了减少假阳性，实验采用去除小连通域的方法进行后处理，将 3D 分割结果图中体积小于阈值的区域置为背景，阈值为分割结果图中的最大连通域的体积的三分之一。



# 集成 

![](https://pic2.zhimg.com/v2-e32f0053d7b8b8e0aa178658aaf16a15_r.jpg)


使用不同随机种子的学习网络F1，…F10 —— 尽管具有非常相似的测试性能 —— 被观察到与非常不同的函数相关联。实际上，使用一种著名的技术叫做集成(ensemble)，只需对这些独立训练的网络的输出进行无加权的平均，就可以在许多深度学习应用中获得测试时性能的巨大提升。(参见下面的图1。)这意味着单个函数F1，…F10必须是不同的。然而，为什么集成的效果会突然提高呢？另外，如果一个人直接训练(F1+⋯+F10)/10，为什么性能提升会消失？

# 知识蒸馏 

尽管集成在提高测试时性能方面非常出色，但在推理时间(即测试时间)上，它的速度会慢10倍：我们需要计算10个神经网络的输出，而不是一个。当我们在低能耗、移动环境中部署这样的模型时，这是一个问题。为了解决这个问题，提出了一种叫做知识蒸馏的开创性技术。也就是说，知识蒸馏只需要训练另一个单独的模型就可以匹配集成的输出。在这里，对猫图像的集成输出(也称为“dark knowledge”)可能是类似“80% cat + 10% dog + 10% car”，而真正的训练标签是“100% cat”

事实证明，经过训练的单个模型，在很大程度上，可以匹配10倍大的集成测试时的表现。

![](https://pic4.zhimg.com/v2-ccabe6da75bd07b9a9f023464612f907_b.jpg)


# 构架


## Mlp-mixer


下图 1 描述了 Mixer 的宏观架构，它以一系列图像块的线性投影（输入的形状为 patches × channels）作为输入，先将输入图片拆分为 patch，通过 Per-patch Fully-connected 将每个 patch 转换为 feature embedding，接着馈入 N 个 Mixer Layer，最后通过 Fully-connected 进行分类。

![](https://pic2.zhimg.com/v2-391f4f65b4e534dd56b9662a2b4e20c9_r.jpg)

- Mixer 架构采用两种不同类型的 MLP 层：channel-mixing MLP 和 token-mixing MLP。

- channel-mixing MLP 允许不同通道之间进行通信，token-mixing MLP 允许不同空间位置（token）之间进行通信。这两种类型的层交替执行以促进两个维度间的信息交互。

- Mixer 架构可以看做是一个特殊的 CNN，使用 1×1 卷积进行 channel mixing，同时全感受野和参数共享的的单通道深度卷积进行 token mixing。

### 设计思想

- Mixer 架构的设计思想是**清楚地**将按位置（channel-mixing）操作 (i) 和跨位置（token-mixing）操作 (ii) 分开，两种操作都通过 MLP 来实现。

- Mixer 由大小相同的多个层组成。每个层由 2 个 MLP 块组成，其中，第一个块是 token-mixing MLP 块，第二个是 channel-mixing MLP 块。每个 MLP 块包含两个全连接层，以及一个单独应用于其输入数据张量的每一行的非线性层。

- Mixer 中的每个层（初始 patch 投影层除外）都采用相同大小的输入，这种「各向同性（isotropic）」的设计与使用固定宽度的 Transformer 或其他域中的深度 RNN 大致相似。

- 这不同于大多数具有金字塔结构的 CNN，即较深的层具有较低分辨率的输入，但是有较多通道（channel）。

- 除了 MLP 层，Mixer 还使用其他标准架构组件：跳远连接（skip-connection）和层归一化。此外，和 ViT 不同，Mixer 不使用位置嵌入，因为 token-mixing MLP 对输入 token 的顺序很敏感，因此能够学会表征位置。最后，Mixer 将标准分类头与全局平均池化层配合使用，随后使用线性分类器。




### 1*1 卷积

![](https://pic4.zhimg.com/80/v2-cf42a8eb42fa902347a619c6e4878840_1440w.jpg?source=3af55fa1)

1. 调节通道数
1. 增加非线性，可以在保持特征图尺度不变的（即不改变）的前提下大幅增加非线性特性
1. 跨通道信息，通道间的信息交互。
1. 减少参数，

## Transformer


自注意力通过使用所有位置之间的成对亲和力来计算特征的加权总和，以捕获单个样本内的长期依赖性，从而更新每个位置的特征。

![](https://pic3.zhimg.com/v2-a1d3c1ac443534c7f5a1f5693d4ea04e_r.jpg)



### ViT（vision transformer）
Google在2020年
1. 直接把图像分成固定大小的patchs，然后通过线性变换得到patch embedding，这就类比NLP的words和word embedding
1. 由于transformer的输入就是a sequence of token embeddings，所以将图像的patch embeddings送入transformer后就能够进行特征提取从而分类了




## DenseNet

![](https://pic2.zhimg.com/v2-c81da515c8fa9796601fde82e4d36f61_r.jpg)

- 它的基本思路与ResNet一致，但是它建立的是前面所有层与后面层的密集连接（dense connection）
- 可以看到，ResNet是每个层与前面的某层（一般是2~3层）短路连接在一起，连接方式是通过元素级相加。而在DenseNet中，每个层都会与前面所有层在channel维度上连接（concat）在一起（这里各个层的特征图大小是相同的，后面会有说明），并作为下一层的输入。

![](https://pic3.zhimg.com/v2-862e1c2dcb24f10d264544190ad38142_r.jpg)
**ResNet**
![](https://pic3.zhimg.com/v2-2cb01c1c9a217e56c72f4c24096fe3fe_r.jpg)
**DenseNet**

![](https://pic2.zhimg.com/v2-0a9db078f505b469973974aee9c27605_r.jpg)
**DenseNet forward**

- CNN网络一般要经过Pooling或者stride>1的Conv来降低特征图的大小，而DenseNet的密集连接方式需要特征图大小保持一致。为了解决这个问题，DenseNet网络中使用DenseBlock+Transition的结构，其中DenseBlock是包含很多层的模块，每个层的特征图大小相同，层与层之间采用密集连接方式。而Transition模块是连接两个相邻的DenseBlock，并且通过Pooling使特征图大小降低。图4给出了DenseNet的网路结构，它共包含4个DenseBlock，各个DenseBlock之间通过Transition连接在一起。


![](https://pic4.zhimg.com/v2-ed66515594a04be849a8cee707cb83bf_r.jpg)

![](https://pic4.zhimg.com/v2-0b28a49f274da9bd8dec2dccddf1ec53_r.jpg)
***DenseNet的网络结构***