---
layout: post
title: loss函数
date: 2021-5-11
author: 
tags: [cv, note]
comments: true
toc: true
pinned: False
---

<!-- more -->

# loss函数

## 设计原则
1. 能够表示网络输出和待分割目标的相似程度
1. Loss的计算过程是可导的，可以进行误差反传

## Cross-Entropy (CE)
Cross-Entropy和它的一些改进说起，这一类Loss函数可以叫做Pixel-Level的Loss，因为他们都是把分割问题看做对每个点的分类

### wCE (weighted cross-entropy）
1. 对边界像素这些难学习的像素加大权重（pixel-weight）
1. 做类层面的加权（class-weight），比如对前景像素加大权重，背景像素减小权重，或者不同的类别按照其所占像素的比例，分配权重，占比小的权重大一些，占比大的权重小一些，解决样本不平衡的问题。

### focal loss
1. 使难分类样本权重大，易分类样本权重小。至于哪些是难分类样本哪些是易分类样本，都由网络的输出和真实的偏差决定。这就实现了**网络自适应调整**。类比我们学知识，难学习的内容，我们同样时间学的不好自己知道，我就会自觉的花更多精力去学习。以前的神经网络没有识别难易任务自动分配精力的方式，focal loss带来了这种自适应反馈。

### Dice Loss
1. Dice系数是分割效果的一个评判指标，其公式相当于预测结果区域和ground truth区域的交并比，所以它是把一个类别的所有像素作为一个整体去计算Loss的。因为Dice Loss直接把分割效果评估指标作为Loss去监督网络，不绕弯子，而且计算交并比时还忽略了大量背景像素，解决了正负样本不均衡的问题，所以收敛速度很快。


## 分割任务

**语义分割中一般用交叉熵来做损失函数，而评价的时候却使用IOU来作为评价指标**
1. 首先采用交叉熵损失函数，而非 dice-coefficient 和类似 IoU 度量的损失函数，一个令人信服的愿意是其梯度形式更优：
1. 直接采用 dice-coefficient 或者 IoU 作为损失函数的原因，是因为分割的真实目标就是最大化 dice-coefficient 和 IoU 度量. 而交叉熵仅是一种代理形式，利用其在 BP 中易于最大化优化的特点.
1. **Dice Loss** 训练误差曲线非常混乱，很难看出关于收敛的信息。尽管可以检查在验证集上的误差来避开此问题。