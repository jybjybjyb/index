---
layout: post
title: Net design tips
date: 2021-5-11
author: 
tags: [cv, note]
comments: true
toc: true
pinned: False
---

<!-- more -->

# 网络设计

## BN batch-normalization

## 添加残差连接

# Opt优化



# Loss函数

## 设计原则
1. 能够表示网络输出和待分割目标的相似程度
1. Loss的计算过程是可导的，可以进行误差反传

## Cross-Entropy (CE)
Cross-Entropy和它的一些改进说起，这一类Loss函数可以叫做Pixel-Level的Loss，因为他们都是把分割问题看做对每个点的分类

### wCE (weighted cross-entropy）
1. 对边界像素这些难学习的像素加大权重（pixel-weight）
1. 做类层面的加权（class-weight），比如对前景像素加大权重，背景像素减小权重，或者不同的类别按照其所占像素的比例，分配权重，占比小的权重大一些，占比大的权重小一些，解决样本不平衡的问题。

### Focal Loss
1. 使难分类样本权重大，易分类样本权重小。至于哪些是难分类样本哪些是易分类样本，都由网络的输出和真实的偏差决定。这就实现了**网络自适应调整**。类比我们学知识，难学习的内容，我们同样时间学的不好自己知道，我就会自觉的花更多精力去学习。以前的神经网络没有识别难易任务自动分配精力的方式，focal loss带来了这种自适应反馈。

### Dice Loss
1. Dice系数是分割效果的一个评判指标，其公式相当于预测结果区域和ground truth区域的交并比，所以它是把一个类别的所有像素作为一个整体去计算Loss的。因为Dice Loss直接把分割效果评估指标作为Loss去监督网络，不绕弯子，而且计算交并比时还忽略了大量背景像素，解决了正负样本不均衡的问题，所以收敛速度很快。


## 分割任务

**语义分割中一般用交叉熵来做损失函数，而评价的时候却使用IOU来作为评价指标**
1. 首先采用交叉熵损失函数，而非 dice-coefficient 和类似 IoU 度量的损失函数，一个令人信服的愿意是其梯度形式更优：
1. 直接采用 dice-coefficient 或者 IoU 作为损失函数的原因，是因为分割的真实目标就是最大化 dice-coefficient 和 IoU 度量. 而交叉熵仅是一种代理形式，利用其在 BP 中易于最大化优化的特点.
1. **Dice Loss** 训练误差曲线非常混乱，很难看出关于收敛的信息。尽管可以检查在验证集上的误差来避开此问题。

### Dics Loss

- 医学图像分割任务中常用的 Dice 损失函数
- Dice 相似系数（DSC，Dice Similarity Coefficient）
- 

### T-test

# 预处理


## Patch Size and Batch Size

在三维医学图像的处理中，显存不足是经常容易遇到的问题。而训练过程中，显存占用与模型的输入patch size，batch size以及网络的结构相关。然而，patch size越大，所需要的模型越大。因此，在抉择时，主要在patch size与batch size之间进行权衡。nnUNet中优先考虑patch size，保证模型能基于更多的信息来进行推理。但batch size最小值应该为2，需要保证训练过程中优化过程的鲁棒性。最后，在保证patch size的前提下，如果显存有多余的，再对应增加batch size。因为batch size大部分情况都比较小，所以之前固定设计中没有用BN而使用Instance Norm。

而最优patch size的决定，采用迭代的方式进行。将batch size先固定为2，然后设定一个较大的patch size初始值，再根据当前patch size来设计网络结构。之后根据显存占用情况，不断的迭代调整patch size。

## target spacing

target spacing指的是数据集在经过重采样之后，每张图像所具有的相同的spacing值，即每张图像的每个体素所代表的实际物理空间大小。
## 体积阈值化
为了减少假阳性，实验采用去除小连通域的方法进行后处理，将 3D 分割结果图中体积小于阈值的区域置为背景，阈值为分割结果图中的最大连通域的体积的三分之一。



# 集成 

![](https://pic2.zhimg.com/v2-e32f0053d7b8b8e0aa178658aaf16a15_r.jpg)


使用不同随机种子的学习网络F1，…F10 —— 尽管具有非常相似的测试性能 —— 被观察到与非常不同的函数相关联。实际上，使用一种著名的技术叫做集成(ensemble)，只需对这些独立训练的网络的输出进行无加权的平均，就可以在许多深度学习应用中获得测试时性能的巨大提升。(参见下面的图1。)这意味着单个函数F1，…F10必须是不同的。然而，为什么集成的效果会突然提高呢？另外，如果一个人直接训练(F1+⋯+F10)/10，为什么性能提升会消失？

# 知识蒸馏 

尽管集成在提高测试时性能方面非常出色，但在推理时间(即测试时间)上，它的速度会慢10倍：我们需要计算10个神经网络的输出，而不是一个。当我们在低能耗、移动环境中部署这样的模型时，这是一个问题。为了解决这个问题，提出了一种叫做知识蒸馏的开创性技术。也就是说，知识蒸馏只需要训练另一个单独的模型就可以匹配集成的输出。在这里，对猫图像的集成输出(也称为“dark knowledge”)可能是类似“80% cat + 10% dog + 10% car”，而真正的训练标签是“100% cat”

事实证明，经过训练的单个模型，在很大程度上，可以匹配10倍大的集成测试时的表现。

![](https://pic4.zhimg.com/v2-ccabe6da75bd07b9a9f023464612f907_b.jpg)

